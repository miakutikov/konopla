#!/usr/bin/env python3
"""
youtube_monitor.py ‚Äî –ú–æ–Ω—ñ—Ç–æ—Ä–∏—Ç—å YouTube –¥–ª—è –Ω–æ–≤–∏—Ö –≤—ñ–¥–µ–æ –ø—Ä–æ –ø—Ä–æ–º–∏—Å–ª–æ–≤—ñ –∫–æ–Ω–æ–ø–ª—ñ.

–®—É–∫–∞—î –≤—ñ–¥–µ–æ —á–µ—Ä–µ–∑ YouTube Data API v3, —Ä–µ—Ä–∞–π—Ç–∏—Ç—å –æ–ø–∏—Å —á–µ—Ä–µ–∑ Gemini,
—Ç–∞ —Å—Ç–≤–æ—Ä—é—î draft-—Å—Ç–∞—Ç—Ç—ñ –∑ youtube_id —É frontmatter.
"""

import json
import os
import sys
import time
import urllib.request
import urllib.error
from datetime import datetime, timedelta, timezone

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from config import API_DELAY_SECONDS
from rewriter import rewrite_article
from publisher import create_article_file

PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
PROCESSED_FILE = os.path.join(PROJECT_ROOT, "data", "processed_videos.json")
DRAFTS_FILE = os.path.join(PROJECT_ROOT, "data", "drafts.json")
CONTENT_DIR = os.path.join(PROJECT_ROOT, "content", "news")

YOUTUBE_API_KEY = os.environ.get("YOUTUBE_API_KEY", "")

# Maximum videos to process per run (conserve API quota)
MAX_VIDEOS_PER_RUN = 3

# How many days to look back for videos
SEARCH_DAYS_BACK = 7

# Search queries for industrial hemp content
SEARCH_QUERIES = [
    "–ø—Ä–æ–º–∏—Å–ª–æ–≤—ñ –∫–æ–Ω–æ–ø–ª—ñ",
    "hemp ukraine",
    "–∫–æ–Ω–æ–ø–ª—è–Ω–∞ —ñ–Ω–¥—É—Å—Ç—Ä—ñ—è",
    "hempcrete",
    "–∫–æ–Ω–æ–ø–ª—è–Ω–∏–π –±–µ—Ç–æ–Ω",
    "hemp textile",
    "industrial hemp",
    "–∫–æ–Ω–æ–ø–ª—è–Ω–µ –≤–æ–ª–æ–∫–Ω–æ",
]


def load_processed():
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Å–ø–∏—Å–æ–∫ –æ–±—Ä–æ–±–ª–µ–Ω–∏—Ö –≤—ñ–¥–µ–æ."""
    if os.path.exists(PROCESSED_FILE):
        with open(PROCESSED_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"video_ids": []}


def save_processed(data):
    """–ó–±–µ—Ä—ñ–≥–∞—î —Å–ø–∏—Å–æ–∫ –æ–±—Ä–æ–±–ª–µ–Ω–∏—Ö –≤—ñ–¥–µ–æ."""
    os.makedirs(os.path.dirname(PROCESSED_FILE), exist_ok=True)
    # Keep only last 500 entries
    data["video_ids"] = data["video_ids"][-500:]
    with open(PROCESSED_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def load_json(filepath, default):
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î JSON —Ñ–∞–π–ª."""
    if os.path.exists(filepath):
        with open(filepath, "r", encoding="utf-8") as f:
            return json.load(f)
    return default


def save_json(filepath, data):
    """–ó–±–µ—Ä—ñ–≥–∞—î JSON —Ñ–∞–π–ª."""
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def youtube_api_request(endpoint, params):
    """–†–æ–±–∏—Ç—å –∑–∞–ø–∏—Ç –¥–æ YouTube Data API v3."""
    params["key"] = YOUTUBE_API_KEY

    query_string = "&".join(f"{k}={urllib.request.quote(str(v))}" for k, v in params.items())
    url = f"https://www.googleapis.com/youtube/v3/{endpoint}?{query_string}"

    req = urllib.request.Request(url)
    req.add_header("Accept", "application/json")

    try:
        with urllib.request.urlopen(req, timeout=30) as resp:
            return json.loads(resp.read().decode("utf-8"))
    except urllib.error.HTTPError as e:
        error_body = e.read().decode("utf-8", errors="replace")
        print(f"[ERROR] YouTube API {e.code}: {error_body[:300]}")
        return None
    except Exception as e:
        print(f"[ERROR] YouTube API request failed: {e}")
        return None


def search_videos(query, published_after, max_results=5):
    """–®—É–∫–∞—î –≤—ñ–¥–µ–æ –Ω–∞ YouTube –∑–∞ –∑–∞–ø–∏—Ç–æ–º."""
    params = {
        "part": "snippet",
        "q": query,
        "type": "video",
        "order": "date",
        "publishedAfter": published_after,
        "relevanceLanguage": "uk",
        "maxResults": max_results,
    }

    data = youtube_api_request("search", params)
    if not data:
        return []

    return data.get("items", [])


def get_video_details(video_id):
    """–û—Ç—Ä–∏–º—É—î –¥–µ—Ç–∞–ª—ñ –≤—ñ–¥–µ–æ (–ø–æ–≤–Ω–∏–π –æ–ø–∏—Å, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É)."""
    params = {
        "part": "snippet,statistics,contentDetails",
        "id": video_id,
    }

    data = youtube_api_request("videos", params)
    if not data or not data.get("items"):
        return None

    return data["items"][0]


def is_hemp_relevant(title, description):
    """–ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ –≤—ñ–¥–µ–æ –¥—ñ–π—Å–Ω–æ –ø—Ä–æ –ø—Ä–æ–º–∏—Å–ª–æ–≤—ñ –∫–æ–Ω–æ–ø–ª—ñ."""
    text = f"{title} {description}".lower()

    # Must contain at least one hemp keyword
    hemp_keywords = [
        "hemp", "–∫–æ–Ω–æ–ø–ª—ñ", "–∫–æ–Ω–æ–ø–ª—è", "–∫–æ–Ω–æ–ø–ª—è–Ω–∏–π", "–∫–æ–Ω–æ–ø–ª—è–Ω–µ",
        "hempcrete", "–ø—Ä–æ–º–∏—Å–ª–æ–≤—ñ", "industrial", "textile", "—Ç–µ–∫—Å—Ç–∏–ª—å",
        "–≤–æ–ª–æ–∫–Ω–æ", "fiber", "fibre", "–±—É–¥—ñ–≤–Ω–∏—Ü—Ç–≤–æ", "construction",
        "–±—ñ–æ–ø–ª–∞—Å—Ç–∏–∫", "bioplastic", "–∞–≥—Ä–æ", "farming",
    ]

    has_hemp = any(kw in text for kw in hemp_keywords)
    if not has_hemp:
        return False

    # Must NOT be about drugs
    drug_keywords = [
        "marijuana", "–º–∞—Ä–∏—Ö—É–∞–Ω–∞", "weed", "ganja",
        "stoner", "420", "dispensary", "psychoactive",
        "narcotic", "–Ω–∞—Ä–∫–æ—Ç–∏–∫", "drug bust", "get high",
        "thc oil", "thc gummies", "delta-8",
        "medical marijuana", "indica", "sativa",
    ]

    has_drug = any(kw in text for kw in drug_keywords)
    return not has_drug


def run_youtube_monitor():
    """–û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É YouTube."""
    print("=" * 60)
    print("üé¨ KONOPLA.UA ‚Äî YouTube Monitor")
    print("=" * 60)

    if not YOUTUBE_API_KEY:
        print("[ERROR] YOUTUBE_API_KEY not set. Exiting.")
        return 1

    processed = load_processed()
    processed_ids = set(processed["video_ids"])

    cutoff = datetime.now(timezone.utc) - timedelta(days=SEARCH_DAYS_BACK)
    published_after = cutoff.strftime("%Y-%m-%dT%H:%M:%SZ")

    print(f"[INFO] Searching videos published after {published_after}")

    # Collect candidate videos from all queries
    candidates = {}  # video_id -> snippet data
    for query in SEARCH_QUERIES:
        print(f"\nüîç Query: {query}")
        results = search_videos(query, published_after, max_results=5)

        for item in results:
            video_id = item.get("id", {}).get("videoId")
            if not video_id:
                continue
            if video_id in processed_ids:
                print(f"  [SKIP] Already processed: {video_id}")
                continue
            if video_id in candidates:
                continue

            snippet = item.get("snippet", {})
            title = snippet.get("title", "")
            description = snippet.get("description", "")

            if not is_hemp_relevant(title, description):
                print(f"  [SKIP] Not relevant: {title[:60]}")
                continue

            candidates[video_id] = snippet
            print(f"  [OK] Candidate: {title[:60]}")

        time.sleep(1)  # Brief pause between queries

    print(f"\n[INFO] Found {len(candidates)} new candidate videos")

    if not candidates:
        print("[INFO] No new videos to process. Done.")
        return 0

    # Process top N candidates
    drafts = load_json(DRAFTS_FILE, {"articles": []})
    count = 0

    for video_id, snippet in list(candidates.items())[:MAX_VIDEOS_PER_RUN]:
        print(f"\n{'=' * 40}")
        print(f"üé¨ Processing video: {video_id}")

        # Get full video details (only 1 API unit!)
        details = get_video_details(video_id)
        if not details:
            print(f"  [WARN] Failed to get details for {video_id}")
            continue

        full_snippet = details.get("snippet", {})
        title = full_snippet.get("title", "")
        description = full_snippet.get("description", "")
        channel = full_snippet.get("channelTitle", "")
        published_at = full_snippet.get("publishedAt", "")
        thumbnail = full_snippet.get("thumbnails", {}).get("high", {}).get("url", "")

        stats = details.get("statistics", {})
        views = stats.get("viewCount", "0")

        print(f"  Title: {title[:70]}")
        print(f"  Channel: {channel}")
        print(f"  Views: {views}")

        # Rewrite via Gemini with video-specific context
        video_url = f"https://www.youtube.com/watch?v={video_id}"
        video_content = f"""–¶–µ YouTube –≤—ñ–¥–µ–æ –ø—Ä–æ –ø—Ä–æ–º–∏—Å–ª–æ–≤—ñ –∫–æ–Ω–æ–ø–ª—ñ.

–ù–∞–∑–≤–∞: {title}
–ö–∞–Ω–∞–ª: {channel}
–û–ø–∏—Å: {description[:2000]}
–ü–µ—Ä–µ–≥–ª—è–¥—ñ–≤: {views}
–ü–æ—Å–∏–ª–∞–Ω–Ω—è: {video_url}

–í–ê–ñ–õ–ò–í–û: –¶–µ –≤—ñ–¥–µ–æ, –∞ –Ω–µ —Ç–µ–∫—Å—Ç–æ–≤–∞ —Å—Ç–∞—Ç—Ç—è. –ù–∞–ø–∏—à–∏ –∞–Ω–æ–Ω—Å/–æ–≥–ª—è–¥ —Ü—å–æ–≥–æ –≤—ñ–¥–µ–æ –¥–ª—è —Å–∞–π—Ç—É Konopla.UA.
–ó–∞–∑–Ω–∞—á —â–æ —Ü–µ –≤—ñ–¥–µ–æ —ñ —Ö—Ç–æ –π–æ–≥–æ –∞–≤—Ç–æ—Ä. –ó–∞–æ—Ö–æ—Ç—å —á–∏—Ç–∞—á–∞ –ø–µ—Ä–µ–≥–ª—è–Ω—É—Ç–∏ –≤—ñ–¥–µ–æ –Ω–∞ —Å–∞–π—Ç—ñ."""

        try:
            print("  ‚úçÔ∏è Rewriting...")
            rewritten = rewrite_article(
                title=title,
                summary=description[:500],
                source_url=video_url,
                content=video_content,
            )
        except Exception as e:
            print(f"  ‚ùå Rewrite error: {e}")
            rewritten = None

        if not rewritten:
            print(f"  ‚è≠Ô∏è Skipping ‚Äî rewrite failed")
            processed["video_ids"].append(video_id)
            continue

        # Force category to "–≤—ñ–¥–µ–æ"
        rewritten["category"] = "–≤—ñ–¥–µ–æ"
        # Add youtube_id
        rewritten["youtube_id"] = video_id

        # Use thumbnail as image
        image_data = None
        if thumbnail:
            image_data = {
                "url": thumbnail,
                "author": channel,
                "author_url": f"https://www.youtube.com/@{channel.replace(' ', '')}",
                "unsplash_url": video_url,
                "source": "youtube",
            }

        print(f"  ‚úÖ {rewritten['title'][:60]}...")

        # Create draft file
        filepath = create_article_file(
            article_data=rewritten,
            source_url=video_url,
            source_name=f"YouTube: {channel}",
            image_data=image_data,
            content_dir=CONTENT_DIR,
            draft=True,
        )

        if filepath:
            import uuid
            article_id = str(uuid.uuid4())[:8]
            filename = os.path.basename(filepath)

            drafts["articles"].append({
                "id": article_id,
                "filename": filename,
                "title": rewritten.get("title", ""),
                "summary": rewritten.get("summary", ""),
                "category": "–≤—ñ–¥–µ–æ",
                "image": thumbnail,
                "created_at": datetime.now(timezone.utc).isoformat(),
            })

            count += 1
            print(f"  üìù Draft created: {filename}")

        processed["video_ids"].append(video_id)

        if count < MAX_VIDEOS_PER_RUN:
            time.sleep(API_DELAY_SECONDS)

    # Save everything
    save_processed(processed)
    save_json(DRAFTS_FILE, drafts)

    print(f"\n{'=' * 60}")
    print(f"üìä YouTube Monitor complete!")
    print(f"   üé¨ Candidates found: {len(candidates)}")
    print(f"   ‚úçÔ∏è Drafts created: {count}")
    print("=" * 60)

    # Notify admin via Telegram
    try:
        from telegram_bot import send_message, ADMIN_CHAT_ID
        if count > 0:
            send_message(
                f"üé¨ <b>YouTube Monitor</b>\n\n"
                f"–ó–Ω–∞–π–¥–µ–Ω–æ {count} –Ω–æ–≤–∏—Ö –≤—ñ–¥–µ–æ.\n"
                f"üëâ <a href=\"https://konopla.ua/admin/\">–ú–æ–¥–µ—Ä—É–≤–∞—Ç–∏</a>",
                chat_id=ADMIN_CHAT_ID
            )
    except Exception as e:
        print(f"[WARN] Admin notification failed: {e}")

    return 0


if __name__ == "__main__":
    exit_code = run_youtube_monitor()
    sys.exit(exit_code)
